import gradio as gr
import pandas as pd
import asyncio
import json
from datetime import datetime
from src.services import api_config_service, dataset_service, llm_service


def create_generation_ui():
    """åˆ›å»ºè¯­æ–™ç”ŸæˆUI"""

    # --- Helper Functions ---
    def load_form_from_config(config_name):
        if not config_name:
            return "", "", "", 1.0, 0.0, 0.0, "OpenAI", gr.update(visible=True)
        config = api_config_service.get_api_config_by_name(config_name)
        if config:
            is_visible = config["api_type"] in ["OpenAI", "Anthropic"]
            return (
                config["name"],
                config["api_key"],
                config.get("base_url", ""),
                config.get("top_p", 1.0),
                config.get("frequency_penalty", 0.0),
                config.get("presence_penalty", 0.0),
                config["api_type"],
                gr.update(visible=is_visible),
            )
        return (
            gr.update(),
            gr.update(),
            gr.update(),
            gr.update(),
            gr.update(),
            gr.update(),
            gr.update(),
            gr.update(),
        )

    def load_form_and_models(config_name):
        """Load form data and fetch models when config changes"""
        # Get form data
        form_data = load_form_from_config(config_name)

        # Get models
        if config_name:
            try:
                models = api_config_service.get_available_models(config_name)
                model_update = gr.update(
                    choices=models, value=models[0] if models else None
                )
                gr.Info(f"å·²è‡ªåŠ¨è·å– {len(models)} ä¸ªå¯ç”¨æ¨¡å‹")
            except Exception as e:
                model_update = gr.update(choices=[])

        # Return form data + model update
        return form_data + (model_update,)

    def refresh_all_configs():
        configs = api_config_service.get_all_api_configs()
        names = [c["name"] for c in configs]
        df_data = [
            {
                "é…ç½®åç§°": c["name"],
                "APIç±»å‹": c["api_type"],
                "Base URL": c.get("base_url", ""),
                "åˆ›å»ºæ—¶é—´": c["created_at"],
            }
            for c in configs
        ]
        df = pd.DataFrame(df_data)
        return gr.update(choices=names), df

    def on_save_api_config(name, provider, key, base_url, top_p, freq_p, pres_p):
        if not all([name, provider, key]):
            gr.Warning("é…ç½®åç§°ã€APIæä¾›å•†å’ŒAPI Keyä¸èƒ½ä¸ºç©ºï¼")
            return gr.update(), gr.update()
        try:
            api_config_service.save_api_config(
                name, provider, key, base_url, top_p, freq_p, pres_p
            )
            gr.Info(f"APIé…ç½® '{name}' å·²æˆåŠŸä¿å­˜ã€‚")
            return refresh_all_configs()
        except ValueError as e:
            gr.Warning(str(e))
            return gr.update(), gr.update()

    def on_delete_api_config(config_name_to_delete):
        if not config_name_to_delete:
            gr.Warning("è¯·å…ˆä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªè¦åˆ é™¤çš„é…ç½®ã€‚")
            return gr.update(), gr.update()
        api_config_service.delete_api_config(config_name_to_delete)
        gr.Info(f"APIé…ç½® '{config_name_to_delete}' å·²åˆ é™¤ã€‚")
        return refresh_all_configs()

    def on_get_models(config_name):
        """Fetches and updates the model dropdown."""
        if not config_name:
            gr.Warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªAPIé…ç½®ã€‚")
            return gr.update(choices=[])
        try:
            gr.Info(f"æ­£åœ¨ä» {config_name} è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
            models = api_config_service.get_available_models(config_name)
            gr.Info(f"æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹ã€‚")
            # Update choices, set the first model as default, and ensure it's interactive
            if models:
                return gr.update(choices=models, value=models[0], interactive=True)
            else:
                gr.Warning("æœªèƒ½è·å–åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ã€‚")
                return gr.update(choices=[], interactive=True)
        except Exception as e:
            gr.Warning(str(e))
            return gr.update(choices=[], interactive=True)

    def load_datasets():
        datasets = dataset_service.get_all_datasets_for_display()
        return gr.update(choices=[d["name"] for d in datasets], interactive=True)

    def start_generation(
        dataset_name,
        api_config_name,
        model_name,
        num_to_generate,
        conversation_turns,
        parallel_requests,
        temperature,
        max_tokens,
        top_p,
        frequency_penalty,
        presence_penalty,
    ):
        """å¼€å§‹ç”Ÿæˆè¯­æ–™"""
        if not all([dataset_name, api_config_name, model_name]):
            gr.Warning("è¯·ç¡®ä¿å·²é€‰æ‹©æ•°æ®é›†ã€APIé…ç½®å’Œæ¨¡å‹ï¼")
            return "è¯·å®Œå–„ç”Ÿæˆé…ç½®", gr.update(), None

        try:
            # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
            progress_msg = f"å¼€å§‹ç”Ÿæˆ {num_to_generate} æ¡è¯­æ–™...\n"
            progress_msg += f"æ•°æ®é›†: {dataset_name}\n"
            progress_msg += f"APIé…ç½®: {api_config_name}\n"
            progress_msg += f"æ¨¡å‹: {model_name}\n"
            progress_msg += f"å¹¶è¡Œè¯·æ±‚æ•°: {parallel_requests}\n"

            # è¿è¡Œå¼‚æ­¥ç”Ÿæˆä»»åŠ¡
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                batch = loop.run_until_complete(
                    llm_service.generate_corpus_batch(
                        dataset_name=dataset_name,
                        api_config_name=api_config_name,
                        model_name=model_name,
                        total_count=num_to_generate,
                        conversation_turns=conversation_turns,
                        parallel_requests=parallel_requests,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        top_p=top_p,
                        frequency_penalty=frequency_penalty,
                        presence_penalty=presence_penalty,
                    )
                )

                # æ ¼å¼åŒ–ç»“æœ
                total_time = (batch.end_time - batch.start_time).total_seconds()
                progress_msg += "\nâœ… ç”Ÿæˆå®Œæˆï¼\n"
                progress_msg += f"æˆåŠŸ: {batch.completed}/{batch.total_requested}\n"
                progress_msg += f"å¤±è´¥: {batch.failed}\n"
                progress_msg += f"ç”¨æ—¶: {total_time:.2f}ç§’\n"

                # å‡†å¤‡é¢„è§ˆæ•°æ®
                preview_data = []
                for i, conversation in enumerate(batch.results[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
                    scenarios_str = ", ".join(conversation.get("scenarios", []))
                    dialogues = conversation.get("dialogues", [])

                    dialogue_text = ""
                    for turn in dialogues:
                        role = turn.get("role", "unknown")
                        content = turn.get("content", "")
                        dialogue_text += f"{role}: {content}\n"

                    preview_data.append(
                        {
                            "åºå·": i + 1,
                            "åœºæ™¯æ ‡ç­¾": scenarios_str,
                            "å¯¹è¯å†…å®¹": dialogue_text.strip(),
                            "è½®æ•°": len(dialogues),
                        }
                    )

                preview_df = pd.DataFrame(preview_data)

                # å­˜å‚¨åˆ°å…¨å±€çŠ¶æ€ä»¥ä¾›ç¡®è®¤å…¥åº“ä½¿ç”¨
                current_batch_state = {"batch": batch, "dataset_name": dataset_name}

                gr.Info(f"ç”Ÿæˆå®Œæˆï¼æˆåŠŸç”Ÿæˆ {batch.completed} æ¡å¯¹è¯")
                return progress_msg, preview_df, current_batch_state

            finally:
                loop.close()

        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            gr.Warning(error_msg)
            return error_msg, gr.update(), None

    def confirm_save_results(current_batch_state):
        """ç¡®è®¤ä¿å­˜ç”Ÿæˆç»“æœåˆ°æ•°æ®åº“"""
        if not current_batch_state or "batch" not in current_batch_state:
            gr.Warning("æ²¡æœ‰å¯ä¿å­˜çš„ç”Ÿæˆç»“æœï¼")
            return "æ²¡æœ‰å¯ä¿å­˜çš„ç”Ÿæˆç»“æœ"

        try:
            batch = current_batch_state["batch"]
            dataset_name = current_batch_state["dataset_name"]

            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = llm_service.save_generation_results(batch, dataset_name)

            success_msg = f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡è¯­æ–™åˆ°æ•°æ®é›† '{dataset_name}'"
            gr.Info(success_msg)
            return success_msg

        except Exception as e:
            error_msg = f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"
            gr.Warning(error_msg)
            return error_msg

    def export_results_as_json(current_batch_state):
        """å¯¼å‡ºç”Ÿæˆç»“æœä¸ºJSONæ–‡ä»¶"""
        if not current_batch_state or "batch" not in current_batch_state:
            return None

        try:
            batch = current_batch_state["batch"]

            # å‡†å¤‡å¯¼å‡ºæ•°æ®
            export_data = {
                "batch_info": {
                    "batch_id": batch.batch_id,
                    "dataset_name": batch.dataset_name,
                    "character_name": batch.character_name,
                    "scenario_names": batch.scenario_names,
                    "total_requested": batch.total_requested,
                    "completed": batch.completed,
                    "failed": batch.failed,
                    "start_time": batch.start_time.isoformat(),
                    "end_time": batch.end_time.isoformat() if batch.end_time else None,
                },
                "conversations": batch.results,
            }

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"generation_results_{batch.batch_id}_{timestamp}.json"

            # å†™å…¥æ–‡ä»¶
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            gr.Info(f"ç»“æœå·²å¯¼å‡ºåˆ°æ–‡ä»¶: {filename}")
            return filename

        except Exception as e:
            gr.Warning(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
            return None

    # --- UI Definition ---
    with gr.Blocks(analytics_enabled=False) as generation_ui:
        selected_config_name_state = gr.State(None)
        current_batch_state = gr.State(None)  # å­˜å‚¨å½“å‰ç”Ÿæˆæ‰¹æ¬¡çš„çŠ¶æ€

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. é€‰æ‹©æ•°æ®é›†")
                target_dataset = gr.Dropdown(label="é€‰æ‹©ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œç”Ÿæˆ")

                gr.Markdown("### 2. é…ç½®ç”Ÿæˆå‚æ•°")
                with gr.Group():
                    num_to_generate = gr.Slider(
                        label="æ€»ç”Ÿæˆæ•°é‡", minimum=1, maximum=1000, step=1, value=10
                    )
                    conversation_turns = gr.Slider(
                        label="æ¯æ¡è¯­æ–™å¯¹è¯è½®æ•°", minimum=1, maximum=20, step=1, value=3
                    )
                    parallel_requests = gr.Slider(
                        label="å¹¶è¡Œè¯·æ±‚æ•°",
                        minimum=1,
                        maximum=20,
                        step=1,
                        value=1,
                        info="åŒæ—¶å‘é€ç»™LLMçš„è¯·æ±‚æ•°é‡",
                    )

                gr.Markdown("### 3. é…ç½®APIè°ƒç”¨")
                with gr.Group():
                    api_config = gr.Dropdown(label="é€‰æ‹©APIé…ç½®")
                    with gr.Row():
                        model_name = gr.Dropdown(label="é€‰æ‹©æ¨¡å‹", scale=4)
                        get_models_btn = gr.Button(
                            "ğŸ”„ è·å–å¯ç”¨æ¨¡å‹", scale=1, min_width=50
                        )
                    temperature = gr.Slider(
                        label="æ¸©åº¦", minimum=0.0, maximum=2.0, step=0.1, value=0.7
                    )
                    max_tokens = gr.Slider(
                        label="æœ€å¤§é•¿åº¦", minimum=8096, maximum=128000, step=64, value=2048
                    )
                    top_p = gr.Slider(
                        label="Top P",
                        minimum=0.0,
                        maximum=1.0,
                        step=0.05,
                        value=1.0,
                    )
                    frequency_penalty = gr.Slider(
                        label="é¢‘ç‡æƒ©ç½š",
                        minimum=-2.0,
                        maximum=2.0,
                        step=0.1,
                        value=0.5,
                    )
                    presence_penalty = gr.Slider(
                        label="å­˜åœ¨æƒ©ç½š",
                        minimum=-2.0,
                        maximum=2.0,
                        step=0.1,
                        value=0.5,
                    )
                    with gr.Accordion("ç®¡ç†APIé…ç½®", open=False):
                        with gr.Tabs():
                            with gr.TabItem("æ·»åŠ /æ›´æ–°APIé…ç½®"):
                                api_provider = gr.Dropdown(
                                    label="APIæä¾›å•†",
                                    choices=["OpenAI", "Google", "Anthropic"],
                                    value="OpenAI",
                                )
                                api_name = gr.Textbox(
                                    label="é…ç½®åç§°",
                                    info="ä¸ºè¿™ä¸ªAPIé…ç½®èµ·ä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚",
                                )
                                api_key = gr.Textbox(label="API Key", type="password")
                                base_url = gr.Textbox(
                                    label="Base URL (å¯é€‰)", visible=True
                                )
                                with gr.Row():
                                    save_api_btn = gr.Button("ğŸ’¾ ä¿å­˜å¹¶æµ‹è¯•")

                            with gr.TabItem("å·²å­˜é…ç½®åˆ—è¡¨"):
                                api_config_list = gr.Dataframe(
                                    headers=[
                                        "é…ç½®åç§°",
                                        "APIç±»å‹",
                                        "Base URL",
                                        "åˆ›å»ºæ—¶é—´",
                                    ],
                                    datatype=["str", "str", "str", "str"],
                                    row_count=5,
                                    label="å·²ä¿å­˜çš„APIé…ç½®åˆ—è¡¨",
                                    interactive=True,
                                )
                                with gr.Row():
                                    refresh_configs_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                                    delete_config_btn = gr.Button(
                                        "ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­é…ç½®", variant="stop"
                                    )

            with gr.Column(scale=2):
                gr.Markdown("### 4. é¢„è§ˆä¸å¯åŠ¨")
                with gr.Group():
                    with gr.Tabs():
                        with gr.TabItem("ğŸ“ æœ€ç»ˆæç¤ºè¯é¢„è§ˆ"):
                            prompt_preview = gr.TextArea(
                                label="ç”Ÿæˆçš„æç¤ºè¯å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ",
                                lines=20,
                                interactive=True,
                                placeholder='é€‰æ‹©å¥½å‚æ•°åï¼Œç‚¹å‡»"ç”Ÿæˆ/åˆ·æ–°æç¤ºè¯"æŒ‰é’®è¿›è¡Œé¢„è§ˆã€‚',
                            )
                    with gr.Row():
                        generate_prompt_btn = gr.Button("âš™ï¸ ç”Ÿæˆ/åˆ·æ–°æç¤ºè¯")
                        start_generation_btn = gr.Button(
                            "ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary"
                        )

                gr.Markdown("### 5. ç»“æœé¢„è§ˆä¸å®¡æ ¸")
                with gr.Group():
                    generation_status = gr.TextArea(
                        label="ç”ŸæˆçŠ¶æ€",
                        lines=6,
                        placeholder="ç”ŸæˆçŠ¶æ€ä¿¡æ¯å°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                        interactive=False,
                    )

                    results_preview = gr.Dataframe(
                        headers=["åºå·", "åœºæ™¯æ ‡ç­¾", "å¯¹è¯å†…å®¹", "è½®æ•°"],
                        datatype=["number", "str", "str", "number"],
                        label="ç”Ÿæˆç»“æœé¢„è§ˆ (æœ€å¤šæ˜¾ç¤ºå‰10æ¡)",
                        interactive=False,
                        wrap=True,
                    )

                    with gr.Row():
                        save_results_btn = gr.Button("ğŸ’¾ ç¡®è®¤å…¥åº“", variant="primary")
                        export_json_btn = gr.Button("ğŸ“„ å¯¼å‡ºJSON")

        # --- Event Handlers Binding ---
        api_form_outputs = [
            api_name,
            api_key,
            base_url,
            top_p,
            frequency_penalty,
            presence_penalty,
            api_provider,
            base_url,
        ]
        all_sliders = [
            num_to_generate,
            conversation_turns,
            parallel_requests,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
        ]

        # On page load, load datasets, configs, and explicitly enable all sliders
        def initial_load():
            datasets_update = load_datasets()
            configs_update, configs_df_update = refresh_all_configs()
            # Return update for each slider to ensure they are interactive
            slider_updates = [gr.update(interactive=True)] * len(all_sliders)
            return datasets_update, configs_update, configs_df_update, *slider_updates

        generation_ui.load(
            initial_load,
            outputs=[target_dataset, api_config, api_config_list, *all_sliders],
        )

        # Main API Config Dropdown Event
        api_config.change(
            load_form_from_config, inputs=api_config, outputs=api_form_outputs
        )

        # --- Events for "æ·»åŠ /æ›´æ–°APIé…ç½®" Tab ---
        def update_visibility(provider):
            return gr.update(visible=provider in ["OpenAI", "Anthropic"])

        api_provider.change(update_visibility, inputs=api_provider, outputs=base_url)
        save_api_btn.click(
            on_save_api_config,
            inputs=[
                api_name,
                api_provider,
                api_key,
                base_url,
                top_p,
                frequency_penalty,
                presence_penalty,
            ],
            outputs=[api_config, api_config_list],
        )

        get_models_btn.click(on_get_models, inputs=api_config, outputs=model_name)

        # --- Events for "å·²å­˜é…ç½®åˆ—è¡¨" Tab ---
        refresh_configs_btn.click(
            refresh_all_configs, outputs=[api_config, api_config_list]
        )

        def on_select_config_in_list(df: pd.DataFrame, evt: gr.SelectData):
            if evt.index is None:
                return None
            name = df.iloc[evt.index[0]]["é…ç½®åç§°"]
            return name

        api_config_list.select(
            on_select_config_in_list,
            [api_config_list],
            selected_config_name_state,
            queue=False,
        ).then(load_form_from_config, selected_config_name_state, api_form_outputs)
        delete_config_btn.click(
            on_delete_api_config,
            selected_config_name_state,
            [api_config, api_config_list],
        )

        # Event for the "Generate Preview" button
        generate_prompt_btn.click(
            fn=llm_service.generate_preview_prompt,
            inputs=[target_dataset, conversation_turns, num_to_generate],
            outputs=[prompt_preview],
        )

        # Event for the "Start Generation" button
        start_generation_btn.click(
            fn=start_generation,
            inputs=[
                target_dataset,
                api_config,
                model_name,
                num_to_generate,
                conversation_turns,
                parallel_requests,
                temperature,
                max_tokens,
                top_p,
                frequency_penalty,
                presence_penalty,
            ],
            outputs=[generation_status, results_preview, current_batch_state],
        )

        # Event for the "Save Results" button
        save_results_btn.click(
            fn=confirm_save_results,
            inputs=[current_batch_state],
            outputs=[generation_status],
        )

        # Event for the "Export JSON" button
        export_json_btn.click(
            fn=export_results_as_json,
            inputs=[current_batch_state],
            outputs=None,  # File download handled internally
        )

    return generation_ui
