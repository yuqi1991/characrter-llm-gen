import logging
import json
import asyncio
import aiohttp
from datetime import datetime
from typing import List, Dict, Optional, Any
from string import Template
from openai import OpenAI, AsyncOpenAI
from google import genai
from pydantic import BaseModel, Field
from src.services import dataset_service, character_service, api_config_service
import os
import glob

logger = logging.getLogger(__name__)


# Pydantic models for structured output
class DialogueTurn(BaseModel):
    """å•è½®å¯¹è¯ç»“æ„"""

    role: str = Field(description="å‘è¨€è€…è§’è‰²ï¼š'user' æˆ– 'assistant'")
    content: str = Field(description="å¯¹è¯å†…å®¹")


class ConversationItem(BaseModel):
    """å•æ¡å¯¹è¯ç»“æ„"""

    scenarios: List[str] = Field(description="å¯¹è¯æ¶‰åŠçš„åœºæ™¯æ ‡ç­¾åˆ—è¡¨")
    dialogues: List[DialogueTurn] = Field(description="å¯¹è¯è½®æ¬¡åˆ—è¡¨")


class GenerationResult(BaseModel):
    """ç”Ÿæˆç»“æœç»“æ„"""

    conversations: List[ConversationItem] = Field(description="ç”Ÿæˆçš„å¯¹è¯åˆ—è¡¨")


class GenerationBatch(BaseModel):
    """ç”Ÿæˆæ‰¹æ¬¡ä¿¡æ¯"""

    batch_id: str
    dataset_name: str
    character_name: str
    scenario_names: List[str]
    total_requested: int
    completed: int = 0
    failed: int = 0
    results: List[Dict[str, Any]] = []
    start_time: datetime
    end_time: Optional[datetime] = None


def get_prompt_templates() -> List[Dict[str, str]]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿æ–‡ä»¶åˆ—è¡¨"""
    try:
        template_dir = "templates/prompts"
        if not os.path.exists(template_dir):
            return [
                {"name": "é»˜è®¤æ¨¡æ¿", "path": "templates/prompts/generation_prompt.txt"}
            ]

        # è·å–æ‰€æœ‰.txtæ–‡ä»¶
        pattern = os.path.join(template_dir, "*.txt")
        template_files = glob.glob(pattern)

        templates = []
        for file_path in template_files:
            filename = os.path.basename(file_path)
            templates.append({"name": filename, "path": file_path})

        # æŒ‰åç§°æ’åºï¼Œç¡®ä¿é»˜è®¤æ¨¡æ¿åœ¨å‰é¢
        templates.sort(key=lambda x: (x["name"] != "é»˜è®¤æ¨¡æ¿", x["name"]))
        return templates
    except Exception as e:
        logger.error(f"è·å–æ¨¡æ¿æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return [{"name": "é»˜è®¤æ¨¡æ¿", "path": "templates/prompts/generation_prompt.txt"}]


def read_prompt_template(
    file_path="templates/prompts/generation_prompt.txt",
) -> Template:
    """Reads the prompt template file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return Template(f.read())
    except FileNotFoundError:
        logger.error(f"æç¤ºè¯æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        raise


def generate_preview_prompt(
    dataset_name: str,
    conversation_turns: int,
    num_to_generate: int,
    template_path: str = "templates/prompts/generation_prompt.txt",
) -> str:
    """
    Generates the final prompt for LLM based on a dataset and parameters.
    """
    if not dataset_name:
        return "âš ï¸ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ•°æ®é›†å†é¢„è§ˆæç¤ºè¯ã€‚\n\nğŸ“ ä½¿ç”¨è¯´æ˜ï¼š\n1. åœ¨å·¦ä¾§é¢æ¿é€‰æ‹©ä¸€ä¸ªæ•°æ®é›†\n2. é€‰æ‹©åˆé€‚çš„æç¤ºè¯æ¨¡æ¿\n3. è°ƒæ•´å¯¹è¯è½®æ•°å’Œç”Ÿæˆæ•°é‡\n4. ç‚¹å‡»æ­¤æŒ‰é’®é¢„è§ˆæœ€ç»ˆæç¤ºè¯"

    # 1. Get dataset details, which includes linked character and scenarios
    try:
        dataset = dataset_service.get_dataset_details(dataset_name)
        if not dataset:
            return f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®é›† '{dataset_name}'ã€‚è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ã€‚"

        character_name = dataset.get("character_name")
        if not character_name:
            return f"âŒ é”™è¯¯ï¼šæ•°æ®é›† '{dataset_name}' æœªç»‘å®šä»»ä½•è§’è‰²ã€‚è¯·å…ˆä¸ºæ•°æ®é›†é…ç½®è§’è‰²ã€‚"

        # 2. Get full character details
        character = character_service.get_character_by_name(character_name)
        if not character:
            return f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è§’è‰²è¯¦æƒ… '{character_name}'ã€‚è¯·æ£€æŸ¥è§’è‰²æ˜¯å¦å­˜åœ¨ã€‚"

        # 3. Get scenario details and format them into a list string
        scenarios = dataset.get("scenario_objects", [])
        scenarios_list_str = "\n".join(
            f"\n- {i+1}. {s['name']}:\n {s['description'].replace('{{char}}', character_name)}"
            for i, s in enumerate(scenarios)
        )
        if not scenarios_list_str:
            scenarios_list_str = "General conversation without specific scenarios."

        # 4. Read the template with specified path
        template = read_prompt_template(template_path)

        # 5. Substitute the template with data
        prompt_data = {
            "character_name": character.get("name", ""),
            "character_personality": character.get("personality", ""),
            "character_background": character.get("background", ""),
            "character_speaking_style": character.get("speaking_style", ""),
            "conversation_turns": conversation_turns,
            "scenarios_list": scenarios_list_str,
            "dialogue_examples": character.get("dialogue_examples", "N/A"),
            "num_to_generate": num_to_generate,
        }

        final_prompt = template.substitute(prompt_data)
        return final_prompt

    except Exception as e:
        logger.error(f"ç”Ÿæˆé¢„è§ˆæç¤ºè¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return f"âŒ ç”Ÿæˆé¢„è§ˆæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥ï¼š\n- æ•°æ®é›†æ˜¯å¦æ­£ç¡®é…ç½®\n- è§’è‰²ä¿¡æ¯æ˜¯å¦å®Œæ•´\n- åœºæ™¯è®¾ç½®æ˜¯å¦æ­£ç¡®\n- æ¨¡æ¿æ–‡ä»¶æ˜¯å¦å­˜åœ¨"


async def call_openai_structured(
    client: AsyncOpenAI,
    prompt: str,
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 2048,
    top_p: float = 1.0,
    frequency_penalty: float = 0.5,
    presence_penalty: float = 0.5,
    **kwargs,
) -> Dict[str, Any]:
    """è°ƒç”¨OpenAI APIå¹¶è¦æ±‚ç»“æ„åŒ–è¾“å‡º"""
    try:
        logger.debug(prompt)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "è¯·ä½ æ‰§è¡Œä»¥ä¸‹ç”¨æˆ·è¦æ±‚çš„ä»»åŠ¡",
                },
                {"role": "user", "content": prompt},
            ],
            # response_format=GenerationResult,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            **kwargs,
        )

        if response.choices[0].message.content:
            logger.debug(response.choices[0].message.content)
            try:
                content = response.choices[0].message.content
                clean_string = content.removeprefix("```json\n").removesuffix("\n```")
                json_content = json.loads(clean_string)  # ç›´æ¥è§£æJSON
                if isinstance(json_content, list):
                    return {"conversations": json_content}
                elif isinstance(json_content, dict) and "conversations" in json_content:
                    return json_content
                else:
                    return {"conversations": []}
            except json.JSONDecodeError:
                logger.error(
                    f"æ— æ³•è§£æå“åº”ä¸ºJSON: \n {response.choices[0].message.content}"
                )
                return {"conversations": []}
        else:
            # Fallback to regular content if parsing fails
            content = response.choices[0].message.content
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                logger.error(f"æ— æ³•è§£æå“åº”ä¸ºJSON: {content}")
                return {"conversations": []}

    except Exception as e:
        logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        raise


async def call_google_structured(
    api_key: str,
    prompt: str,
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 2048,
    **kwargs,
) -> Dict[str, Any]:
    """è°ƒç”¨Google AI APIå¹¶è§£æç»“æ„åŒ–è¾“å‡º"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model)

        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
        )

        response = await model.generate_content_async(
            prompt, generation_config=generation_config
        )

        content = response.text
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            logger.error(f"æ— æ³•è§£æGoogleå“åº”ä¸ºJSON: {content}")
            return {"conversations": []}

    except Exception as e:
        logger.error(f"Google AI APIè°ƒç”¨å¤±è´¥: {e}")
        raise


async def generate_single_batch(
    api_config: Dict[str, Any],
    prompt: str,
    model: str,
    temperature: float,
    max_tokens: int,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
) -> Dict[str, Any]:
    """ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„å¯¹è¯"""
    api_type = api_config["api_type"]
    api_key = api_config["api_key"]
    base_url = api_config.get("base_url")

    if api_type == "OpenAI":
        client = AsyncOpenAI(
            api_key=api_key.strip(), base_url=base_url.strip() if base_url else None
        )
        return await call_openai_structured(
            client,
            prompt,
            model,
            temperature,
            max_tokens,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
        )
    elif api_type == "Google":
        return await call_google_structured(
            api_key, prompt, model, temperature, max_tokens
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„APIç±»å‹: {api_type}")


async def generate_corpus_batch(
    dataset_name: str,
    api_config_name: str,
    model_name: str,
    num_to_generate: int,
    conversation_turns: int,
    total_requests: int,
    max_parallel_requests: int,
    batch_cooldown_seconds: int,
    temperature: float = 0.7,
    max_tokens: int = 8096,
    top_p: float = 1.0,
    frequency_penalty: float = 0.5,
    presence_penalty: float = 0.5,
    template_path: str = "templates/prompts/generation_prompt.txt",
    progress_callback=None,
) -> GenerationBatch:
    """å¼‚æ­¥æ‰¹é‡ç”Ÿæˆè¯­æ–™æ•°æ®"""

    # è·å–APIé…ç½®
    api_config = api_config_service.get_api_config_by_name(api_config_name)
    if not api_config:
        raise ValueError(f"APIé…ç½® '{api_config_name}' ä¸å­˜åœ¨")

    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset = dataset_service.get_dataset_details(dataset_name)
    if not dataset:
        raise ValueError(f"æ•°æ®é›† '{dataset_name}' ä¸å­˜åœ¨")

    # ç”ŸæˆåŸºç¡€æç¤ºè¯
    base_prompt = generate_preview_prompt(
        dataset_name, conversation_turns, num_to_generate, template_path
    )  # æ¨¡æ¿ä¸­ç”¨1ï¼Œåç»­ä¼šæ›¿æ¢

    # åˆ›å»ºæ‰¹æ¬¡ä¿¡æ¯
    batch = GenerationBatch(
        batch_id=f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        dataset_name=dataset_name,
        character_name=dataset.get("character_name", ""),
        scenario_names=[s["name"] for s in dataset.get("scenario_objects", [])],
        total_requested=total_requests * num_to_generate,
        start_time=datetime.now(),
    )

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for i in range(total_requests):
        task = generate_single_batch(
            api_config=api_config,
            prompt=base_prompt,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
        )
        await asyncio.sleep(1)  # é˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
        tasks.append(task)
        logger.info(f"ç”Ÿæˆä»»åŠ¡ {i+1} å·²æ·»åŠ : è¯·æ±‚ç”Ÿæˆ{num_to_generate} æ¡è¯­æ–™")
        if i % max_parallel_requests == 0:
            await asyncio.sleep(batch_cooldown_seconds)

    logger.info(
        f"å¼€å§‹å¹¶è¡Œç”Ÿæˆ {len(tasks)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ¬¡è¯·æ±‚ç”Ÿæˆ {num_to_generate} æ¡å¯¹è¯ï¼Œæœ€å¤§å¹¶è¡Œè¯·æ±‚æ•° {max_parallel_requests}ï¼Œ\
            å†·å´æ—¶é—´ {batch_cooldown_seconds} ç§’ï¼Œæ€»è¯·æ±‚æ•°é‡ {total_requests}"
    )

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    try:
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹æ¬¡ {i+1} ç”Ÿæˆå¤±è´¥: {result}")
                batch.failed += 1
                if progress_callback:
                    progress_callback(f"æ‰¹æ¬¡ {i+1} å¤±è´¥: {str(result)}")
            else:
                conversations = result.get("conversations", [])
                batch.results.extend(conversations)
                batch.completed += len(conversations)
                logger.info(f"æ‰¹æ¬¡ {i+1} æˆåŠŸç”Ÿæˆ {len(conversations)} æ¡å¯¹è¯")
                if progress_callback:
                    progress_callback(
                        f"æ‰¹æ¬¡ {i+1} å®Œæˆï¼Œç”Ÿæˆ {len(conversations)} æ¡å¯¹è¯"
                    )

    except Exception as e:
        logger.error(f"æ‰¹é‡ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        batch.failed = len(tasks)
        if progress_callback:
            progress_callback(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    batch.end_time = datetime.now()
    total_time = (batch.end_time - batch.start_time).total_seconds()
    logger.info(
        f"æ‰¹æ¬¡ {batch.batch_id} å®Œæˆ: æˆåŠŸ {batch.completed}/{batch.total_requested}ï¼Œç”¨æ—¶ {total_time:.2f}ç§’"
    )

    return batch


def save_generation_results(batch: GenerationBatch, dataset_name: str) -> int:
    """å°†ç”Ÿæˆç»“æœä¿å­˜åˆ°æ•°æ®åº“"""
    from src.services.dataset_service import save_corpus_to_dataset

    saved_count = 0
    for conversation in batch.results:
        try:
            # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
            dialogue_data = {
                "scenario_labels": conversation.get("scenarios", []),
                "dialogues": [
                    {
                        "role": turn.get("role", "user"),
                        "content": turn.get("content", ""),
                    }
                    for turn in conversation.get("dialogues", [])
                ],
                "turn_count": len(conversation.get("dialogues", [])) // 2,
                "batch_id": batch.batch_id,
                "generation_time": batch.start_time.isoformat(),
            }

            scenario_names = conversation.get("scenarios", [])
            save_corpus_to_dataset(dataset_name, dialogue_data, scenario_names)
            saved_count += 1

        except Exception as e:
            logger.error(f"ä¿å­˜å¯¹è¯å¤±è´¥: {e}")

    logger.info(
        f"æˆåŠŸä¿å­˜ {saved_count}/{len(batch.results)} æ¡å¯¹è¯åˆ°æ•°æ®é›† '{dataset_name}'"
    )
    return saved_count
